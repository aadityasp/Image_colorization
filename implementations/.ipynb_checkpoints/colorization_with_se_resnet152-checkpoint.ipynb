{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200k training images(81% of total )\n",
    "#45k test images (18% of total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import utils\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from ipywidgets import FloatProgress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://modelzoo.co/model/pretrained-modelspytorch\n",
    "se_resnet = pretrainedmodels.__dict__[\"se_resnet152\"](\n",
    "    num_classes=1000, \n",
    "    pretrained=\"imagenet\"\n",
    ")\n",
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "\n",
    "VALIDATION_SIZE = 0.2\n",
    "# BATCH_SIZE defines the number of samples that will be propagated through the network.\n",
    "BATCH_SIZE = 128 #64 #128 #250 #64\n",
    "\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a \"model\" at checkpoint saved at \"path\" along with the optimizer and the epoch  to start with.\n",
    "def load_checkpoint(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    return model, optimizer, checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale image is the L channel tensor, ab_inout are the AB channels tensor of LAB image, stack them to get LAB image.\n",
    "def convert_to_LAB(grayscale_input, ab_input):\n",
    "    color_image = torch.cat((grayscale_input, ab_input), axis=0).numpy()\n",
    "    #torch shape is C,H,W\n",
    "    # required for matplotlib = H,W,C\n",
    "    color_image = color_image.transpose((1, 2, 0)) \n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lukemelas.github.io/image-colorization.html\n",
    "# Takes in grayscale, ABchannel tensors, and the AB channel groundtruth tensor and returns RGB numpy arrays of grayscale, ABPredicted and ABgroundthruth images.\n",
    "def convert_to_rgb(grayscale_input, ab_input, ab_ground_truth):\n",
    "    predicted_image = convert_to_LAB(grayscale_input, ab_input)\n",
    "    ground_truth_image = convert_to_LAB(grayscale_input, ab_ground_truth)\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    return grayscale_input, predicted_image, ground_truth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper method to display the images.Takes in the grayscale, predicted and groundtruth images as RGB numpy arrays, plots and returns the matplot figure.\n",
    "def display_images(grayscale, pred_image, ground_truth_image):\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    axarr[0].imshow(grayscale, cmap=\"gray\")\n",
    "    axarr[0].set_title(\"Grayscale Image (Model Input)\", fontsize=20)\n",
    "    axarr[1].imshow(pred_image)\n",
    "    axarr[1].set_title(\"RGB Image (Model Output)\", fontsize=20)\n",
    "    axarr[2].imshow(ground_truth_image)\n",
    "    axarr[2].set_title(\"RGB Image (Ground-truth)\", fontsize=20)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder network designed as per the paper. Takes in a 224x224 x1 scaled grayscale image(L channel) a\n",
    "class Encoder(nn.Module):\n",
    "    # https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()    \n",
    "        self.input_ = nn.Conv2d(1, 64, 3, padding=1, stride=2)\n",
    "        self.conv1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_1 = nn.Conv2d(1256, 256, 1)\n",
    "        self.input_ = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 2, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_1(x))\n",
    "        x = F.relu(self.input_(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.tanh(self.conv4(x))\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_resnet = se_resnet.to(device)\n",
    "se_resnet.eval()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.encoder = self.encoder.to(device)\n",
    "\n",
    "        self.decoder = Decoder()\n",
    "        self.decoder = self.decoder.to(device)\n",
    "\n",
    "    def forward(self, encoder_input, feature_input):\n",
    "        encoded_img = self.encoder(encoder_input)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = se_resnet(feature_input)\n",
    "\n",
    "        embedding = embedding.view(-1, 1000, 1, 1)\n",
    "\n",
    "        rows = torch.cat([embedding] * 28, dim=3)\n",
    "        embedding_block = torch.cat([rows] * 28, dim=2)\n",
    "        fusion_block = torch.cat([encoded_img, embedding_block], dim=1)\n",
    "\n",
    "        return self.decoder(fusion_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the pretrained.utils methods\n",
    "load_img = utils.LoadImage()\n",
    "tf_img = utils.TransformImage(se_resnet) \n",
    "\n",
    "# Encoder and se_resnet models take in different HxW images\n",
    "encoder_transform = transforms.Compose([transforms.CenterCrop(224)])\n",
    "se_resnet_transform = transforms.Compose([transforms.CenterCrop(224)])\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Subclass of ImageFolder that separates LAB channels into L and AB channels.\n",
    "    It also transforms the image into the correctly formatted input for se_resnet.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        img_path, _ = self.imgs[index]\n",
    "\n",
    "        img_se_resnet = tf_img(se_resnet_transform(load_img(img_path)))\n",
    "        img = self.loader(img_path)\n",
    "\n",
    "        img_original = encoder_transform(img)\n",
    "        img_original = np.asarray(img_original)\n",
    "\n",
    "        img_lab = rgb2lab(img_original)\n",
    "        img_lab = (img_lab + 128) / 255\n",
    "\n",
    "        img_ab = img_lab[:, :, 1:3]\n",
    "        \n",
    "        img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "\n",
    "        img_gray = rgb2gray(img_original)\n",
    "        img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "\n",
    "        return img_gray, img_ab, img_se_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset(\"../data/imagenet_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "print(num_train)\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(VALIDATION_SIZE * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "# SubsetRandomSampler: Samples elements randomly from a given list of indices, without replacement.\n",
    "train_samp = SubsetRandomSampler(train_idx)\n",
    "valid_samp = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# train_samp, valid_samp = retrieve_training_validation_samplers(\n",
    "#     train_data, \n",
    "#     VALIDATION_SIZE\n",
    "# )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "  train_data, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  sampler=train_samp,\n",
    "  num_workers=4\n",
    ")\n",
    "print(len(train_dataloader))\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "  train_data, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  sampler=valid_samp,\n",
    "  num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model = model.to(device)\n",
    "\n",
    "#Mean Squared Logarithmic Error (MSLE)\n",
    "#Sometimes, one may not want to penalize the model too much for predicting unscaled quantities directly.\n",
    "# Relaxing the penalty on huge differences can be done with the help of Mean Squared Logarithmic Error.\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "\n",
    "\n",
    "criterion = MSLELoss() #nn.MSELoss()\n",
    "#MSLELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints=[]\n",
    "checkpoint_path = \"./checkpoints/SEnet/colorization_senet.pt\"\n",
    "checkpoints.append(checkpoint_path)\n",
    "# model_name='colorization'\n",
    "# model_name = \"{}.pt\".format(model_name)\n",
    "# save_path = os.path.join(checkpoint_path, model_name)\n",
    "\n",
    "\n",
    "# if not os.path.exists(save_path):\n",
    "#     open(save_path, 'w').close()\n",
    "    \n",
    "# model, optimizer, epochs = load_checkpoint(save_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = \"../data/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_without_groundtruth(location):\n",
    "    test_data = ImageDataset(test_root)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n",
    "    img_gray, img_ab, img_se_resnet = iter(test_dataloader).next()\n",
    "    img_gray, img_ab, img_se_resnet = img_gray.to(device), img_ab.to(device), img_se_resnet.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_gray, img_se_resnet)\n",
    "    \n",
    "    for idx in range(1):\n",
    "        grayscale, predicted_image, _ = convert_to_rgb(\n",
    "          img_gray[idx].cpu(), \n",
    "          output[idx].cpu(), \n",
    "          img_ab[idx].cpu()\n",
    "        )\n",
    "\n",
    "        f, axarr = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        axarr[0].imshow(grayscale, cmap=\"gray\")\n",
    "        axarr[0].set_title(\"Grayscale Image (Model Input)\", fontsize=20)\n",
    "        axarr[1].imshow(predicted_image)\n",
    "        axarr[1].set_title(\"RGB Image (Model Output)\", fontsize=20)\n",
    "        plt.imshow(f)\n",
    "        f.savefig(location,facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_with_groundtruth(location):\n",
    "    test_data = ImageDataset(test_root)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=5)\n",
    "    # print('test_data',len(test_dataloader))\n",
    "    batch_size=5\n",
    "    img_gray, img_ab, img_se_resnet = iter(test_dataloader).next()\n",
    "    # plt.imshow(img_gray)\n",
    "    # plt.imshow(img_ab)\n",
    "    # plt.imshow(img_se_resnet)\n",
    "    img_gray, img_ab, img_se_resnet = img_gray.to(device), img_ab.to(device), img_se_resnet.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_gray, img_se_resnet)\n",
    "    # randomList = random.sample(range(len(test_dataloader)),batch_size)\n",
    "    for idx in range(5): # range(5):\n",
    "        grayscale, predicted_image, ground_truth = convert_to_rgb(\n",
    "          img_gray[idx].cpu(), \n",
    "          output[idx].cpu(), \n",
    "          img_ab[idx].cpu()\n",
    "        )\n",
    "        f= display_images(grayscale, predicted_image, ground_truth)\n",
    "        f.savefig(location,facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell only for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_validation = np.Inf # initially set to infinity\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "final_model ='./checkpoints/colorization_final_seResnet.pt'\n",
    "filesize = os.path.getsize(final_model)\n",
    "if filesize !=0:\n",
    "    model, optimizer, epochs = load_checkpoint(final_model, model, optimizer)\n",
    "    EPOCHS -= epochs\n",
    "\n",
    "for i in tqdm(range(EPOCHS), desc=\"Epoch\"):\n",
    "    location = './results/senet/'+'with_groundtruth'+'e'+str(i)\n",
    "    final_model ='./checkpoints/colorization_final_seResnet.pt'\n",
    "    filesize = os.path.getsize(final_model)#checkpoints[-1])\n",
    "#     if filesize !=0:\n",
    "#         model, optimizer, epochs = load_checkpoint(final_model, model, optimizer)\n",
    "\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for img_gray, img_ab, img_se_resnet in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        img_gray, img_ab, img_se_resnet= img_gray.to(device), img_ab.to(device), img_se_resnet.to(device)\n",
    "        #initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        output = model(img_gray, img_se_resnet)\n",
    "        loss = criterion(output, img_ab)\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #computes loss and updates the weights.\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    else:\n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "        model.eval()\n",
    "#     train_losses.append(train_loss) #.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_gray, img_ab, img_se_resnet in tqdm(valid_dataloader, desc=\"Validating\"):\n",
    "            img_gray, img_ab, img_se_resnet = img_gray.to(device), img_ab.to(device), img_se_resnet.to(device)\n",
    "\n",
    "            output = model(img_gray, img_se_resnet)\n",
    "            valid_loss += criterion(output, img_ab)\n",
    "#         val_losses.append(valid_loss)\n",
    "#     train_loss_epoch = train_loss/len(train_dataloader)\n",
    "#     valid_loss_epoch = valid_loss.item()/len(valid_dataloader)\n",
    "    train_loss = train_loss/len(train_dataloader)\n",
    "    valid_loss = valid_loss/len(valid_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)   \n",
    "    print(\"Epoch: {}/{}.. \".format(i+1, EPOCHS))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Training Loss: {:.6f}.. \".format(train_loss))\n",
    "    print(\"Validation Loss: {:.6f}.. \".format(valid_loss))\n",
    "\n",
    "    if valid_loss <= min_validation:\n",
    "        checkpoints.append(checkpoint_path[:-3]+datetime.now().strftime('%Y-%m-%d %H:%M:%S')+'.pt')\n",
    "        fle = Path(checkpoints[-1])\n",
    "        fle.touch(exist_ok=True)\n",
    "        f = open(fle)\n",
    "        \n",
    "        print(\"Validation loss decreased ({:.6f} --> {:.6f}). Saving model.\" \\\n",
    "        .format(\n",
    "          min_validation,\n",
    "          valid_loss\n",
    "        ))\n",
    "        \n",
    "#             torch.save(model.state_dict(), output)\n",
    "#             # torch.save(model.state_dict(), save_path)\n",
    "#             output.close()\n",
    "#             best_eval.early_current_patience = 0\n",
    "        torch.save(\n",
    "          {\n",
    "            \"epoch\": i,\n",
    "            \"model\": model,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": criterion\n",
    "          }, \n",
    "#           checkpoint_path\n",
    "            checkpoints[-1]\n",
    "        )\n",
    "        min_validation = valid_loss\n",
    "        \n",
    "\n",
    "    torch.save(\n",
    "      {\n",
    "        \"epoch\": i,\n",
    "        \"model\": model,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": criterion\n",
    "      }, \n",
    "#           checkpoint_path\n",
    "        final_model\n",
    "    )\n",
    "    run_test_with_groundtruth(location)        \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.plot(val_losses,label=\"val\")\n",
    "    plt.plot(train_losses,label=\"train\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model, and run it on images in the new_images_root location.\n",
    "Run all the above cells except the training cell and then run the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images_root = \"../data/new_images\"\n",
    "# new_images_root = \"../data/test_data\"\n",
    "def run_test_new_images(location):\n",
    "    test_data = ImageDataset(new_images_root)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size= 10) # len(test_data))\n",
    "    # print('test_data',len(test_dataloader))\n",
    "    batch_size=5\n",
    "    img_gray, img_ab, img_inception = iter(test_dataloader).next()\n",
    "    # plt.imshow(img_gray)\n",
    "    # plt.imshow(img_ab)\n",
    "    # plt.imshow(img_inception)\n",
    "    img_gray, img_ab, img_inception = img_gray.to(device), img_ab.to(device), img_inception.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_gray, img_inception)\n",
    "    # randomList = random.sample(range(len(test_dataloader)),batch_size)\n",
    "    for idx in range(10):# len(test_data)): # range(5):\n",
    "        grayscale, predicted_image, ground_truth = convert_to_rgb(\n",
    "          img_gray[idx].cpu(), \n",
    "          output[idx].cpu(), \n",
    "          img_ab[idx].cpu()\n",
    "        )\n",
    "        f= display_images(grayscale, predicted_image, ground_truth)\n",
    "        f.savefig(location+str(idx)+'.png',facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = './colorization_final_efficientnet_old.pt'\n",
    "# final_model ='./colorization_final.pt'\n",
    "# final_model ='./colorization_authorimpl_final.pt'\n",
    "final_model ='./colorization_final_seResnet.pt'\n",
    "# model, optimizer, epochs = load_checkpoint(, model, optimizer)\n",
    "# image_storage_location = '/home/aadityasp/Aditya/MS/sem2_fall2021/CV_PatternRecognition/Project/data/test_outputs/test_data_images_output_seResnet/'\n",
    "image_storage_location = '/home/aadityasp/Aditya/MS/sem2_fall2021/CV_PatternRecognition/Project/data/new_images_outputs/colorization_final_seresnet/'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model = Network()\n",
    "# model = model.to(device)\n",
    "checkpoint = torch.load(final_model)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model, optimizer, epochs =load_checkpoint(final_model,model,optimizer)\n",
    "print(model)\n",
    "\n",
    "run_test_new_images(image_storage_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References\n",
    "#https://arxiv.org/pdf/1712.03400.pdf\n",
    "#https://github.com/baldassarreFe/deep-koalarization\n",
    "#https://pytorch.org/docs/stable/data.html\n",
    "#https://modelzoo.co/model/pretrained-modelspytorch\n",
    "#https://arxiv.org/pdf/1905.11946.pdf\n",
    "# https://lukemelas.github.io/image-colorization.html\n",
    "# https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
    "# https://github.com/lauradang/automatic-image-colorization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
